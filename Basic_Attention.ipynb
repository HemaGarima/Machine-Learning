{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwkAy/Q9J80iWKJ3JRYKMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemaGarima/Machine-Learning/blob/master/Basic_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic attention operation"
      ],
      "metadata": {
        "id": "h7hMRV3TczGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FGW8BNH-cnbG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x , axis = 0):\n",
        "  \"\"\" Calculate softmax function for an array x along specified axis\n",
        "\n",
        "    axis = 0 calculates softmax across rows which means each column sums to 1\n",
        "    axis = 1 calculates softmax across columns which means each row sums to 1\n",
        "  \"\"\"\n",
        "  return np.exp(x)/np.expand_dims(np.sum(np.exp(x),axis = axis ),axis)"
      ],
      "metadata": {
        "id": "a5cYFCMseIOf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Calculating alignment scores"
      ],
      "metadata": {
        "id": "Li6nXHGCf8mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 16\n",
        "attention_size = 10\n",
        "input_length = 5\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Synthetic vectors used to test\n",
        "encoder_states = np.random.randn(input_length , hidden_size)\n",
        "decoder_state = np.random.randn(1 , hidden_size)\n",
        "\n",
        "# Weights for the neural network , theses are typically learned through training\n",
        "# Use these in the alignment functino below as the layer weights\n",
        "layer_1 = np.random.randn(2*hidden_size , attention_size)\n",
        "layer_2 = np.random.randn(attention_size , 1)\n",
        "\n",
        "# Implement this function. Replace None with your code. Solution at the bottom of the notebook\n",
        "def alignment(encoder_states , decoder_state):\n",
        "  # First , concatenate the encoder states and the decoder state\n",
        "  inputs = np.concatenate([encoder_states , np.repeat(decoder_state , input_length, axis=0)], axis = 1)\n",
        "  assert inputs.shape == (input_length , 2*hidden_size)\n",
        "\n",
        "  # Matrix multiplication of the concatenated inputs and layer_1 , with tanh activation\n",
        "  activations = np.tanh(np.matmul(inputs , layer_1))\n",
        "  assert activations.shape == (input_length , attention_size)\n",
        "\n",
        "  # Matrix multiplication of the activations with layer_2. Remember that you don't need tanh here\n",
        "  scores = np.matmul(activations , layer_2)\n",
        "  assert scores.shape == (input_length , 1)\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "lZlIx5hzfycZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to test your alignment function\n",
        "scores = alignment(encoder_states , decoder_state)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCkURXXQjr0p",
        "outputId": "b598dee3-551b-4a96-b151-ec0415a1f3db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.35790943]\n",
            " [5.92373433]\n",
            " [4.18673175]\n",
            " [2.11437202]\n",
            " [0.95767155]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Turning alignment into weights"
      ],
      "metadata": {
        "id": "XObzZ9W-kfMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement softmax function"
      ],
      "metadata": {
        "id": "GW3jU24TlhNO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Weight the encoder output vectors and sum"
      ],
      "metadata": {
        "id": "OyTTJmgtlk7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(encoder_states , decoder_state):\n",
        "  \"\"\" Example function that calculates attention, returns the context vector\n",
        "\n",
        "        Arguments:\n",
        "        encoder_vectors: NxM numpy array, where N is the number of vectors and M is the vector length\n",
        "        decoder_vector: 1xM numpy array, M is the vector length, much be the same M as encoder_vectors\n",
        "  \"\"\"\n",
        "\n",
        "  # First , calculate the alignment scores\n",
        "  scores = alignment(encoder_states , decoder_state)\n",
        "\n",
        "  # Then take the softmax of the alignment scores to get a weight distribution\n",
        "  weights = softmax(scores)\n",
        "\n",
        "  # Multiply each encoder state by its respective weight\n",
        "  weighted_scores = [state*weight for state , weight in zip(encoder_states , weights)]\n",
        "\n",
        "  # Sum up weighted alignement vectors to get the context to get the context vector and return it\n",
        "  context = sum(weighted_scores)\n",
        "  return context\n",
        "\n",
        "context_vector = attention(encoder_states , decoder_state)\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cimydpbhlj3v",
        "outputId": "b03bf16a-7f17-43b9-cdb4-5c75b2e363e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n",
            "  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n",
            " -0.58015282 -0.58294027 -0.75457577  1.32985756]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HvmiRgLnF5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}