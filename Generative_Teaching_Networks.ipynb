{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYHhreLOFDcr8h2PLvidMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemaGarima/Machine-Learning/blob/master/Generative_Teaching_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GTN - Generative Training Network"
      ],
      "metadata": {
        "id": "V8uBnBYnSItT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Objecjtives\n",
        "By the end of this notebook , i should :\n",
        "- 1. Understand the concepts of teaching networks , meta-learning , and neural architecture search , and how they relate to the objective of data augumentation.\n",
        "- 2. Implement and train a GTN on MNIST , and observe how a GTN can accelerate training."
      ],
      "metadata": {
        "id": "2sGkocAkSPqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_bhPlPQEW5",
        "outputId": "52aedf4e-c80c-48ab-8a19-5cdefb8e9d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->higher) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->higher) (1.3.0)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n",
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
            "2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "if 'higher' not in sys.modules:\n",
        "  !pip install higher\n",
        "import higher as higher\n",
        "\n",
        "print(sys.version)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set important parameters\n",
        "learning_rate = 1e-2\n",
        "inner_loop_iterations = 32\n",
        "outer_loop_iterations = 5\n",
        "num_classes = 10\n",
        "\n",
        "noise_size = 64 # size of noise of curriculum vector\n",
        "img_size = 28 # width/height of generated image\n",
        "\n",
        "inner_loop_batch_size = 128\n",
        "outer_loop_batch_size = 128\n",
        "\n",
        "mnist_mean = 0.1307 # for normalizing mnist images\n",
        "mnist_std =  0.3081 # for normalizing mnist images\n",
        "\n",
        "imgs_per_row = num_classes"
      ],
      "metadata": {
        "id": "5RWks1lYUG1i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "- Download the MNISt dataset and organize it into a torch.utils.data.Dataset object. Then apply torchvision.transforms to convert raw PIL images to tensors."
      ],
      "metadata": {
        "id": "6sJqFjESWA9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MNIST transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mnist_mean,),(mnist_std,)),\n",
        "])\n",
        "\n",
        "# Create data splits\n",
        "train = datasets.MNIST('./data',train = True , transform = transform, download = True)\n",
        "train , val = torch.utils.data.random_split(train,[50000 , 10000])\n",
        "test = datasets.MNIST('./data',train = False,transform = transform , download = True)\n",
        "print('Created train , val , and test datasets.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCGs24avV-GO",
        "outputId": "e4512817-6276-4f40-9ccb-4a702d33ef2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17805420.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 486799.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3821760.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2298013.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Created train , val , and test datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "- Now wrap your dataset class in a torch.utils.data.DataLoader class , whch will iterate over batches in training. This class increases memory access bandwidth so retrieving images from your dataset won't be a bottleneck in training. MNIST images are small , so the increase in memory retrieval speed should be relatively trivial."
      ],
      "metadata": {
        "id": "oycb68ABZRI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train , batch_size = outer_loop_batch_size , shuffle = True , drop_last = True , num_workers = 1 , pin_memory = True ,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val , batch_size = outer_loop_batch_size , shuffle = True , drop_last = True , num_workers = 1 , pin_memory = True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test, batch_size = outer_loop_batch_size , shuffle = True , drop_last = True , num_workers = 1 , pin_memory = True,\n",
        ")"
      ],
      "metadata": {
        "id": "O9GJ9NpUZLZs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST classification\n",
        "- In this next section, you'll implement and train a GTN on MNIST classification. Note that the student model for this task is a classifier. To extend GTNs to other datasets, you also want to check out the weight normalization technique in the paper --- for now on MNIST, you don't need to worry about this. Alright, let's get started with the generator and classifier's model architecture!"
      ],
      "metadata": {
        "id": "IR7Q_RV0b-Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "- Let's now build the generator. For this task, the generator will consist of two fully connected blocks (each consisting of a fully connected layer, a leaky ReLU, and a batch normalization layer) and two convolutional blocks (each consisting of a convolutional layer, a batch normalization layer and a leaky ReLU). A tanh layer is applied to this output to center it around 0 with reasonable standard deviation."
      ],
      "metadata": {
        "id": "hqoaBYlBcC1i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEQe0p39bdZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}